// scrapers/scraperManager.js - VERSI√ìN MASIVA
const { Category } = require('../models');
const RPPScraper = require('./sites/rppScraper');
const ElComercioScraper = require('./sites/elComercioScraper');
const LaRepublicaScraper = require('./sites/laRepublicaScraper');
const GestionScraper = require('./sites/gestion21Scraper');
const Peru21Scraper = require('./sites/peru21Scraper');
const CorreoScraper = require('./sites/correoScraper');
const OjoScraper = require('./sites/ojoScraper');
const ExpresoScraper = require('./sites/expresoScraper');
const LaRazonScraper = require('./sites/larazonScraper');
const TromeScraper = require('./sites/tromeScraper');
const DeporScraper = require('./sites/deporScraper');
const AmericaScraper = require('./sites/americaScraper');
class ScraperManager {
  constructor() {
    this.scrapers = [];
  }

  async initializeScrapers() {
    try {
      console.log('üîß Inicializando categor√≠a...');
      let category = await Category.findOne({ where: { name: 'Nacionales' } });
      if (!category) {
        category = await Category.create({
          name: 'Nacionales',
          slug: 'nacionales',
          description: 'Noticias de medios peruanos',
        });
      }
      console.log(`‚úÖ Categor√≠a ID: ${category.id}`);

      // SOLO los scrapers que funcionan bien (sin Reddit por ahora)
      this.scrapers = [
        new RPPScraper(category.id),
        new ElComercioScraper(category.id),
        new LaRepublicaScraper(category.id),
        new GestionScraper(category.id),
        new Peru21Scraper(category.id),
        new CorreoScraper(category.id),
        new OjoScraper(category.id),
        new ExpresoScraper(category.id),      // NUEVO
        new LaRazonScraper(category.id),      // NUEVO
        new TromeScraper(category.id),        // NUEVO
        new DeporScraper(category.id),        // NUEVO
        new AmericaScraper(category.id)       // NUEVO
      ];

      console.log(`‚úÖ ${this.scrapers.length} scrapers peruanos inicializados`);
    } catch (error) {
      console.error('‚ùå Error inicializando scrapers:', error.message);
    }
  }

  // üéØ NUEVO M√âTODO: SCRAPING MASIVO AUTOM√ÅTICO
  async runMassiveScraping(targetCount = 1000) {
    console.log('\nüöÄ ===== INICIANDO SCRAPING MASIVO =====');
    console.log(`üéØ Objetivo: ${targetCount}+ noticias\n`);

    await this.initializeScrapers();

    let totalNews = 0;
    let cycle = 1;
    const maxCycles = 10; // M√°ximo de ciclos para evitar loop infinito
    const allResults = [];

    while (totalNews < targetCount && cycle <= maxCycles) {
      console.log(`\nüîÑ === CICLO ${cycle} ===`);
      console.log(`üìà Progreso: ${totalNews}/${targetCount} noticias\n`);

      let cycleNews = 0;
      const cycleStart = Date.now();

      for (const scraper of this.scrapers) {
        try {
          console.log(`\nüì∞ Ejecutando: ${scraper.name}`);
          const start = Date.now();
          
          const news = await scraper.scrape();
          const duration = ((Date.now() - start) / 1000).toFixed(2);
          
          cycleNews += news.length;
          totalNews += news.length;

          allResults.push({
            ciclo: cycle,
            scraper: scraper.name,
            noticias: news.length,
            tiempo: `${duration}s`,
            totalAcumulado: totalNews
          });

          console.log(`‚úÖ ${scraper.name}: ${news.length} noticias (Total: ${totalNews})`);

          // Pausa entre scrapers
          await this.delay(3000);
          
        } catch (error) {
          console.error(`‚ùå Error en ${scraper.name}:`, error.message);
          allResults.push({
            ciclo: cycle,
            scraper: scraper.name,
            noticias: 0,
            error: error.message
          });
        }
      }

      const cycleDuration = ((Date.now() - cycleStart) / 1000).toFixed(2);
      console.log(`\nüìä Ciclo ${cycle} completado en ${cycleDuration}s`);
      console.log(`üéØ Noticias este ciclo: ${cycleNews}`);
      console.log(`üìà Total acumulado: ${totalNews}/${targetCount}`);

      // Si a√∫n no llegamos al objetivo, hacer otro ciclo
      if (totalNews < targetCount) {
        const remaining = targetCount - totalNews;
        const estimatedCycles = Math.ceil(remaining / cycleNews);
        
        console.log(`\n‚è≥ Necesitamos ~${estimatedCycles} ciclos m√°s`);
        console.log(`üí§ Esperando 2 minutos antes del pr√≥ximo ciclo...`);
        
        await this.delay(120000); // 2 minutos de espera
        cycle++;
      }
    }

    console.log('\nüéâ ===== SCRAPING MASIVO COMPLETADO =====');
    console.log(`‚úÖ TOTAL FINAL: ${totalNews} noticias recolectadas`);
    console.log(`üîÑ Ciclos ejecutados: ${cycle}`);
    console.log(`‚è∞ Tiempo total: ${this.formatTime(allResults)}`);

    // Mostrar resumen por medio
    this.showSummary(allResults);

    return { totalNews, results: allResults };
  }

  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  formatTime(results) {
    const totalSeconds = results.reduce((sum, result) => {
      const time = parseFloat(result.tiempo) || 0;
      return sum + time;
    }, 0);
    
    const minutes = Math.floor(totalSeconds / 60);
    const seconds = Math.floor(totalSeconds % 60);
    return `${minutes}m ${seconds}s`;
  }

  showSummary(results) {
    const summary = {};
    
    results.forEach(result => {
      if (!summary[result.scraper]) {
        summary[result.scraper] = 0;
      }
      summary[result.scraper] += result.noticias;
    });

    console.log('\nüìä RESUMEN POR MEDIO:');
    Object.entries(summary).forEach(([medio, cantidad]) => {
      console.log(`   ${medio}: ${cantidad} noticias`);
    });
  }

  // M√©todos existentes (runAll y runSpecific)
  async runAll() {
    console.log('\nüöÄ ===== INICIANDO SCRAPING NORMAL =====');
    await this.initializeScrapers();

    let totalNews = 0;
    const results = [];

    for (const scraper of this.scrapers) {
      try {
        const start = Date.now();
        console.log(`\nüì∞ Ejecutando: ${scraper.name}`);
        const news = await scraper.scrape();
        const duration = ((Date.now() - start) / 1000).toFixed(2);

        totalNews += news.length;
        results.push({
          scraper: scraper.name,
          newsCount: news.length,
          duration: `${duration}s`
        });

        console.log(`‚úÖ ${scraper.name}: ${news.length} noticias encontradas`);
        await this.delay(2000);
      } catch (err) {
        console.error(`‚ùå Error en ${scraper.name}:`, err.message);
        results.push({
          scraper: scraper.name,
          newsCount: 0,
          error: err.message
        });
      }
    }

    console.log('\nüìä ===== RESUMEN =====');
    console.table(results);
    console.log(`‚úÖ Total: ${totalNews} noticias nuevas\n`);

    return { totalNews, results };
  }

  async runSpecific(scraperName) {
    await this.initializeScrapers();
    const scraper = this.scrapers.find(s => 
      s.name.toLowerCase().includes(scraperName.toLowerCase())
    );
    
    if (!scraper) {
      console.log(`‚ùå Scraper "${scraperName}" no encontrado`);
      return { totalNews: 0, results: [] };
    }

    console.log(`\nüöÄ Ejecutando scraper: ${scraper.name}\n`);
    const news = await scraper.scrape();
    return {
      totalNews: news.length,
      results: [{ scraper: scraper.name, newsCount: news.length }],
    };
  }
}

module.exports = ScraperManager;
